{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88823f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize,sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tag import pos_tag\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9d82165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five fantastic fish flew find faraway functions . Maybe find another five fantastic fish ? Find fish function please !\n"
     ]
    }
   ],
   "source": [
    "#測試資料\n",
    "training_docs = [\"Five fantastic fish flew off to find faraway functions .\",\n",
    "                 \"Maybe find another five fantastic fish ?\",\n",
    "                 \"Find my fish with a function please !\"]\n",
    "merge=' '.join(training_docs)\n",
    "stopwords_list=stopwords.words('english')\n",
    "#print(stopwords_list)\n",
    "def remove_stopwords(text):\n",
    "    correct_words=[]\n",
    "    for i in text.split():\n",
    "        if i in stopwords_list:\n",
    "            continue\n",
    "        else:\n",
    "            correct_words.append(i)\n",
    "    return ' '.join(correct_words)\n",
    "merge=remove_stopwords(merge)\n",
    "print(merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18d6a90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['five fantastic fish flew find faraway functions .', 'maybe find another five fantastic fish ?', 'find fish function please !']\n"
     ]
    }
   ],
   "source": [
    "#定義斷句的函式\n",
    "def word_tokenized(text):\n",
    "    sentence_tokenized=sent_tokenize(text)\n",
    "    word_tokenize=[]\n",
    "    for i in sentence_tokenized:\n",
    "        word_tokenize.append(i.lower())\n",
    "    return word_tokenize\n",
    "merge=word_tokenized(merge)\n",
    "print(merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7463e16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['five', 'fantastic', 'fish', 'flew', 'find', 'faraway', 'functions', 'maybe', 'find', 'another', 'five', 'fantastic', 'fish', 'find', 'fish', 'function', 'please']\n"
     ]
    }
   ],
   "source": [
    "puts=string.punctuation\n",
    "merge_1=[]\n",
    "for i in merge:\n",
    "    for j in i.split(' '):\n",
    "        if j in puts:\n",
    "            continue\n",
    "        else:\n",
    "            merge_1.append(j)\n",
    "print(merge_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad0befd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 'a', 'a', 'v', 'v', 'a', 'n', 'r', 'v', None, None, 'a', 'a', 'v', 'a', 'n', 'n']\n",
      "['five', 'fantastic', 'fish', 'fly', 'find', 'faraway', 'function', 'maybe', 'find', 'another', 'five', 'fantastic', 'fish', 'find', 'fish', 'function', 'please']\n"
     ]
    }
   ],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "#lemmatation\n",
    "def form_speech_dict(pos):\n",
    "    t={}\n",
    "    for i in pos:\n",
    "        t[i[0]]=i[1]\n",
    "    return t\n",
    "def get_part_of_speech(pose):\n",
    "    w=form_speech_dict(pose)\n",
    "    a=[]\n",
    "    for i in pose:\n",
    "        if w[i[0]].startswith('J'):\n",
    "            a.append(wordnet.ADJ)\n",
    "        elif w[i[0]].startswith('V'):\n",
    "            a.append(wordnet.VERB)\n",
    "        elif w[i[0]].startswith('N'):\n",
    "            a.append(wordnet.NOUN)\n",
    "        elif w[i[0]].startswith('R'):\n",
    "            a.append(wordnet.ADV)\n",
    "        else:\n",
    "            a.append(None)\n",
    "    return a\n",
    "s=(get_part_of_speech(pos_tag(merge_1)))\n",
    "print(s)\n",
    "result_sen=[]\n",
    "for i in range(len(s)):\n",
    "    if s[i]!=None:\n",
    "        result_sen.append(lemmatiser.lemmatize(merge_1[i],pos=s[i]))\n",
    "    else:\n",
    "        result_sen.append(lemmatiser.lemmatize(merge_1[i]))\n",
    "print(result_sen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e32891de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: five\n",
      "token: fantastic\n",
      "token: fish\n",
      "token: fly\n",
      "token: find\n",
      "token: faraway\n",
      "token: function\n",
      "token: maybe\n",
      "token: find\n",
      "token: another\n",
      "token: five\n",
      "token: fantastic\n",
      "token: fish\n",
      "token: find\n",
      "token: fish\n",
      "token: function\n",
      "token: please\n",
      "{'five': 0, 'fantastic': 1, 'fish': 2, 'fly': 3, 'find': 4, 'faraway': 5, 'function': 6, 'maybe': 7, 'another': 8, 'please': 9}\n"
     ]
    }
   ],
   "source": [
    "features_dict = dict()\n",
    "index = 0\n",
    "for token in result_sen:\n",
    "    print(\"token: {}\".format(token))\n",
    "    # if not a new word\n",
    "    if token in features_dict:\n",
    "        continue\n",
    "    else:\n",
    "        features_dict[token] = index\n",
    "        index += 1\n",
    "print(features_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37dfa7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 1, 0, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Five dwarfs find five fishes in farway town \"\n",
    "def form_bow_vector(sent):\n",
    "    sent = sent.split()\n",
    "    for i in range(len(sent)): sent[i]=sent[i].lower()\n",
    "    s=(get_part_of_speech(pos_tag(sent)))\n",
    "    sents=[]\n",
    "    for i in range(len(sent)):\n",
    "        if s[i]!=None:\n",
    "            sents.append(lemmatiser.lemmatize(sent[i],pos=s[i]))\n",
    "        else:\n",
    "            sents.append(lemmatiser.lemmatize(sent[i]))\n",
    "    vec = [0]*len(features_dict)\n",
    "    for word in sents:\n",
    "        if word in features_dict.keys():\n",
    "            vec[features_dict[word]]+=1\n",
    "    return vec\n",
    "print(form_bow_vector(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a2dfdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
